\section{Optimization Analysis}
\label {sec-analysis}

The objective of our optimization analysis is to automatically identify which MPI communications to optimize
and  what local computations can be safely overlapped with the communication, through the following three steps.

\begin{enumerate}

\item Analytically identify MPI operations that are potential
  performance bottlenecks based on the modeling of communication cost
  and the execution flow modeling of the entire application described
  in Section~\ref{sec-model}.  In particular, based on the BET
  representation of the user application, this step identifies from the top
  $N$ most time-consuming MPI \emph{calls} those that collectively take $\geq P\%$
  of the overall communication time, where both $N$ and $P$ are
  user-configurable parameters and were set by default with $N=10$ and
  $P=80$.  The selection is accomplished by simply sorting the
  pre-estimated communication time of all MPI calls in the
  \texttt{BET} and then selecting the top ones.  For example, for the
  NAS FT application shown in Figure~\ref{fig:ft_bet}, a single MPI
  call, the \texttt{MPI\_Alltoall} at the bottom of the BET, is
  selected since it takes more than 95\% of the overall communication
  time.

\item For each identified MPI communication to optimize, locate the closest
  enclosing loops of the MPI communication in the \texttt{BET}---for
  example, node\#2 in Figure~\ref{fig:ft_bet} for the NAS FT
  application---to potentially overlap with the communication.  If the
  enclosing loop does not exist, the communication is given up as an
  optimization target.

\item  Suppose comm(I) is the MPI communication being considered for optimization at each loop iteration. 
Apply loop dependence analysis to identify the statements (Before(I)) that compute data to be transferred in comm(I) (that is, those that have dependence into comm(I)), and the statements (After(I)) that  use the data transferred from comm(i) (that is, those that have dependence from comm(I)). Determine whether comm(I) is independent of After(I-1) and Before(I+1); that is, whether
it is safe to overlap comm(I) across loop iterations. If yes, the loop is selected for optimization. 
\end{enumerate}

A key challenge in optimizing large applications is that MPI
communications are often scattered across procedural boundaries, and
the computation that can be overlapped with them is often some
distance away and similarly across abstraction boundaries. By using
the Skope framework and through the BET representation of the whole
user application, we are able to inter-procedurally select MPI
communications as well as their surrounding loops as potential optimization targets.
Then, an optimization pragma,  \texttt{\#pragma cco do}, 
illustrated at line~1 of Figure~\ref{fig:code:ft}, is inserted automatically before each selected code region
to instruct the compiler to perform additional analysis to determine the safety of optimization.

We use loop dependence analysis within the ROSE C/C++/Fortran compiler~\cite {ROSE} to automatically 
determine the safety of the reordering optimization to each selected code
region. By making the compiler inline all function calls within the region, the entire optimization analysis can be fully automated, if the compiler can find the source code of all the functions invoked inside the loop being optimized; when the source code of some of these function are not available for analysis, the compiler would opt to be conservative and deem the optimization unsafe. If such conservativeness is not desirable, the developer can 
insert the following pragmas to provide additional guidance to the loop dependence analysis. 

\begin{enumerate}

\item \texttt{\#pragma cco ignore}, illustrated at line~3 in
  Figure~\ref{fig:code:ft}, which can be inserted before each
  function call that can be safely ignored when performing dependence
  analysis. That is, these function calls will not implicate the
  safety of any reordering optimization.
  % when the function call is not
  %reachable at runtime but involves I/O statements for debugging
  %purposes. 
  Examples of such function calls include the
  $timer\_start()$ and $timer\_stop()$ in Figure~\ref{fig:code:ft}.

\item \texttt{\#pragma cco override}, illustrated at the first line of
  Figure~\ref{fig:annot:a2a} and \ref{fig:annot:ft}, which defines the
  memory side effects of the following function call.  The override
  definitions, when manually specified, allow dependence analysis to proceed
  across procedural boundaries without requiring the source code or the inlining of the functions invoked. 
  %They are also inserted manually but could
  %be automatically generated through the integration of advanced
  %interprocedural side effect analysis~\cite{kennedy:cpld88}.

\end{enumerate}

For this paper, we manually inserted the above annotations to overcome situations where the source code of the callee is unavailable, 
or the low-level
implementation details of the callee are too complex to be accurately
deciphered by traditional compiler dependence analysis (e.g., the underlying implementations of MPI operations). 
%which is a known issue in compiler analysis~\cite{POET:ICPP11}. 
Figure~\ref{fig:annot:a2a}
shows an example override definition for \texttt{MPI\_Alltoall}, where we use the $read$ and $write$ pseudo statements to indicate
read and write memory accesses.  Here based on the domain knowledge of the
application that the data being sent/received have atomic types instead of
user-defined types, the memory side effect of the operation can be expressed as consecutive read and write memory references
 to the communication buffers ($sendbuf$ and $recvbuf$). We similarly composed
  memory side effect definitions for the other MPI operations. 

Traditional loop dependence analysis in compilers is based on the disambiguation of subscripted array references, 
by solving diophantine equations parameterized by the surrounding loop index variables, to determine whether each pair of 
array references may refer to the same memory location at two arbitrary loop iterations~\cite{AK:Book01}. The analysis therefore
becomes ineffective when memory references are not expressed using array notations or when the subscript of an array reference cannot be expressed as a linear combination of the surrounding loop index variables. Additionally, since the compiler does not have any information about the runtime control flow of a program, it assumes all control paths can happen at runtime, and no unknown system calls (e.g., timer\_start and timer\_stop in Figure~\ref{fig:code:ft}) can be reordered. Our annotation mechanisms are provided to developers to optionally overcome such conservativeness when desired. In our experience of optimizing the NAS application benchmarks, we have used these annotations to serve the following purposes. 

\begin{itemize}
\item Define the memory side effects of MPI operations and system calls that can be ignored (e.g., Figure~\ref{fig:annot:a2a} and Figure~\ref{fig:code:ft}), the source code of which is not available to the compiler. These annotations can be reused across MPI applications without additional work by the developers.
%\item When the same array data are declared with different dimensions
%  in the caller and callee, the manual override definition can
%  normalize data accesses by converting linearized array accesses to
%  easier-to-analyze coordinates.

\item Specialize the memory side effect of a function call when only a single runtime code path is known to be executed by the function call,  to prevent the compiler considering all possible code paths through the default inlining mechanism.  For
  example, in NAS FT, the procedure \texttt{fft} has 6 branches for
  solving different dimensions of the FFT problem (0D, 1D, or 2D),
  while only one branch will be taken at each invocation.  By manually
  overriding the default inlining, we can eliminate the unreachable
  branches from being considered.
  Figure~\ref{fig:annot:ft} shows the annotation we used to override the $fft()$
function in NAS FT.  Here the original function have several branches for
different data layout, while the override definition has only 1D
layout that is the target code path to optimize. These annotations can be automated with developer approval by having the compiler directly interpret the modeling output of Skope.



\end{itemize}


\begin{figure}[h]
{\scriptsize
\begin{verbatim}
$cco override
subroutine MPI_Alltoall(sendbuf, sendcount, sendtype,
  > recvbuf, recvcount, recvtype, comm, ierror)
  do i = 1, sendcount
    read sendbuf(i)
  end do
  do i = 1, recvcount
    write recvbuf(i)
  end do
end subroutine
\end{verbatim}
}
\caption{Example: describe the memory side effect of $MPI\_Alltoall()$ for simple data types}
\label{fig:annot:a2a}
\vspace{-.1in}
\end{figure}

\begin{figure}[h]
{\scriptsize
\begin{verbatim}
1  !$cco do
2  do iter = 1, niter
3    !$cco ignore
4    if (timers_enabled) call timer_start(T_evolve)
5    call evolve(u0,u1,twiddle,dims(1,1),dims(2,1),dims(3,1))
6    !$cco ignore
7    if (timers_enabled) call timer_stop(T_evolve)
8    !$cco ignore
9    if (timers_enabled) call timer_start(T_fft)
10   call fft(-1,u1,u2)
11   !$cco ignore
12   if (timers_enabled) call timer_stop(T_fft)
13   !$cco ignore
14   if (timers_enabled) call timer_start(T_checksum)
15   call checksum(iter,u2,dims(1,1),dims(2,1),dims(3,1))
16   !$cco ignore
17   if (timers_enabled) call timer_stop(T_checksum)
18 end do
\end{verbatim}
}
\caption{Example: annotating a loop to optimize for NAS FT}
\label{fig:code:ft}
\end{figure}

\begin{figure}[h]
{\scriptsize
\begin{verbatim}
$cco override
subroutine fft(dir, x1, x2)
  cffts1(-1,dims(1,3),dims(2,3),dims(3,3),x1,x1,scratch)
  transpose_x_yz(3, 2, x1, x2)
  cffts2(-1,dims(1,2),dims(2,2),dims(3,2),x2,x2,scratch)
  cffts1(-1,dims(1,1),dims(2,1),dims(3,1),x2,x2,scratch)
end subroutine
\end{verbatim}
}
\caption{Example: using 1D layout code path to override the default inlining for NAS FT}
\label{fig:annot:ft}
\end{figure}

%\begin{figure}[h]
%{\scriptsize
%\begin{verbatim}
%subroutine transpose_x_yz(l1, l2, xin, xout)
%  call transpose2_local(dims(1,l1),
%  > dims(2,l1)*dims(3,l1),xin,xout)
%  call transpose2_global(xout,xin)
%  call transpose2_finish(dims(1,l1),
%  > dims(2,l1)*dims(3,l1),xin,xout)
%end subroutine
%\end{verbatim}
%}
%\caption{Source code of \texttt{transpose\_x\_yz}}
%\label{fig:code:transpose}
%\end{figure}

%\begin{figure}[h]
%{\scriptsize
%\begin{verbatim}
%subroutine transpose2_global(xin, xout)
%  call mpi_alltoall(xin, ntdivnp/np, dc_type, xout,
%  > ntdivnp/np, dc_type, commslice1, ierr)
%end subroutine
%\end{verbatim}
%}
%\caption{Original source code of \texttt{transpose2\_global}}
%\label{fig:code:transpose2}
%\end{figure}

