\section{Optimization Analysis}
\label {sec-analysis}

The objective of our optimization analysis is to automatically identify which MPI communications to optimize
and  what local computations can be safely overlapped with the communication, through the following three steps.

\begin{enumerate}

\item Analytically identify MPI operations that are potential
  performance bottlenecks based on the modeling of communication cost
  and the execution flow modeling of the entire application described
  in Section~\ref{sec-model}.  In particular, based on the BET
  representation of the user application, this step identifies the top
  $N$ most time-consuming MPI \emph{calls}, which take more than $P\%$
  of the overall communication time, where both $N$ and $P$ are
  user-configurable parameters and were set by default with $N=10$ and
  $P=80$.  The selection is accomplished by simply sorting the
  pre-estimated communication time of all MPI calls in the
  \texttt{BET} and then selecting the top ones.  For example, for the
  NAS FT application shown in Figure~\ref{fig:ft_bet}, a single MPI
  call, the \texttt{MPI\_Alltoall} at the bottom of the BET, is
  selected since it takes more than 95\% of the overall communication
  time.

\item For each identified MPI communication to optimize, select a loop
  of computation that can be potentially overlapped with the
  communication to improve performance, by locating the closest
  enclosing loops of the MPI communication in the \texttt{BET}---for
  example, node\#2 in Figure~\ref{fig:ft_bet} for the NAS FT
  application, to potentially overlap with the communication.  If the
  enclosing loop does not exist, the communication is given up as an
  optimization target.

\item Apply dependence analysis to check the safety of overlapping the
  selected computation and communication.

\end{enumerate}

A key challenge in optimizing large applications is that MPI
communications are often scattered across procedural boundaries and
the computation that can be overlapped with them is often some
distance away and similarly across abstraction boundaries, By using
the Skope framework and through the BET representation of the whole
user application, we are able to inter-procedurally select MPI
communication hot spots as well as their surrounding local
computations as potential optimization targets.
%After using our extended analytical performance modeling of Skope to automatically select 
%the MPI operations to optimize, the loop surrounding each MPI operation can be directly
%located by traversing upwards the BET representation of the application.  
Then, an optimization pragma,  \texttt{\#pragma cco do}, 
illustrated at line~1 of Figure~\ref{fig:code:ft}, is inserted automatically before each selected code region
to instruct the compiler to perform additional analysis to determine the safety of optimization.

We use standard loop dependence analysis within the ROSE C/C++/Fortran compiler~\cite {ROSE} to automatically 
determine the safety of the reordering optimization to each selected code
region and make the compiler inline all function calls within the region when possible; 
that is, when source code of the callee is available. 
For other function calls where inlining is not advisable to the compiler, we insert the following pragmas to provide
additional guidance to the loop dependence analysis. 

\begin{enumerate}

\item \texttt{\#pragma cco ignore}, illustrated at line~3 in
  Figure~\ref{fig:code:ft}, which is manually inserted before each
  function call that can be safely ignored when performing dependence
  analysis. That is, these function calls will not implicate the
  safety of any reordering optimization, when the function call is not
  reachable at runtime but involves I/O statements for debugging
  purposes. Examples of such function calls include the
  $timer\_start()$ and $timer\_stop()$ in Figure~\ref{fig:code:ft}.

\item \texttt{\#pragma cco override}, illustrated at the first line of
  Figure~\ref{fig:annot:a2a} and \ref{fig:annot:ft}, which defines the
  memory side effects of the following function call.  The override
  definitions, if specified, allow dependence analysis to proceed
  across procedural boundaries without actual inlining of the
  procedure implementations. They are also inserted manually but could
  be automatically generated through the integration of advanced
  interprocedural side effect analysis~\cite{kennedy:cpld88}.

\end{enumerate}

The above annotations are currently manually inserted to overcome situations where
traditional function inlining is insufficient to overcome the difficulty
imposed by procedural boundaries, either because the source code of the callee is unavailable 
or the low-level
implementation details of the callee are too complex to be accurately
deciphered by traditional compiler dependence analysis, which is a known issue in compiler analysis~\cite{POET:ICPP11}. 
Figure~\ref{fig:annot:a2a}
shows an example override definition for \texttt{MPI\_Alltoall} in NAS
FT, where we use the $read$ and $write$ pseudo statements to indicate
read and write memory accesses.  Based on the domain knowledge of the
application that send and receive data have atomic types instead of
user-defined types, its memory side effect can be expressed as data
accesses to consequent memories in source and target data.
Figure~\ref{fig:annot:ft} shows another example of the $fft()$
function in NAS FT.  The original function have several branches for
different data layout, while the override definition has only 1D
layout that is the target code path to optimize.

We manually override function inlining according to the following
criteria:

\begin{itemize}

\item The definition of the function is not available or contains too
  many low-level implementation details that are likely to
  overcomplicate the inlined code.  For example, we manually write a
  memory side effect definition for all MPI function calls.

\item The runtime code path of the function call allows the side
  effects of the invocation to be simplified through specialization
  far more than if automatically determined after inlining.  For
  example, in NAS FT, the procedure \texttt{fft} has 6 branches for
  solving different dimensions of the FFT problems (0D, 1D, or 2D),
  while only one branch will be taken for each test.  By manually
  overriding the original definition, we can eliminate the unreachable
  branches.

\item When the same array data are declared with different dimensions
  in the caller and callee, the manual override definition can
  normalize data accesses by converting linearized array accesses to
  easier-to-analyze coordinates.

\end{itemize}


\begin{figure}[h]
{\scriptsize
\begin{verbatim}
1  !$cco do
2  do iter = 1, niter
3    !$cco ignore
4    if (timers_enabled) call timer_start(T_evolve)
5    call volve(u0,u1,twiddle,dims(1,1),dims(2,1),dims(3,1))
6    !$cco ignore
7    if (timers_enabled) call timer_stop(T_evolve)
8    !$cco ignore
9    if (timers_enabled) call timer_start(T_fft)
10   call fft(-1,u1,u2)
11   !$cco ignore
12   if (timers_enabled) call timer_stop(T_fft)
13   !$cco ignore
14   if (timers_enabled) call timer_start(T_checksum)
15   call checksum(iter,u2,dims(1,1),dims(2,1),dims(3,1))
16   !$cco ignore
17   if (timers_enabled) call timer_stop(T_checksum)
18 end do
\end{verbatim}
}
\caption{Source code with directives of the loop in NAS FT to optimize}
\label{fig:code:ft}
\end{figure}

\begin{figure}[h]
{\scriptsize
\begin{verbatim}
$cco override
subroutine fft(dir, x1, x2)
  cffts1(-1,dims(1,3),dims(2,3),dims(3,3),x1,x1,scratch)
  transpose_x_yz(3, 2, x1, x2)
  cffts2(-1,dims(1,2),dims(2,2),dims(3,2),x2,x2,scratch)
  cffts1(-1,dims(1,1),dims(2,1),dims(3,1),x2,x2,scratch)
end subroutine
\end{verbatim}
}
\caption{1D layout code path to override the original $fft()$ definition}
\label{fig:annot:ft}
\end{figure}

\begin{figure}[h]
{\scriptsize
\begin{verbatim}
subroutine transpose_x_yz(l1, l2, xin, xout)
  call transpose2_local(dims(1,l1),
  > dims(2,l1)*dims(3,l1),xin,xout)
  call transpose2_global(xout,xin)
  call transpose2_finish(dims(1,l1),
  > dims(2,l1)*dims(3,l1),xin,xout)
end subroutine
\end{verbatim}
}
\caption{Source code of \texttt{transpose\_x\_yz}}
\label{fig:code:transpose}
\end{figure}

\begin{figure}[h]
{\scriptsize
\begin{verbatim}
subroutine transpose2_global(xin, xout)
  call mpi_alltoall(xin, ntdivnp/np, dc_type, xout,
  > ntdivnp/np, dc_type, commslice1, ierr)
end subroutine
\end{verbatim}
}
\caption{Original source code of \texttt{transpose2\_global}}
\label{fig:code:transpose2}
\end{figure}

\begin{figure}[h]
{\scriptsize
\begin{verbatim}
$cco override
subroutine MPI_Alltoall(sendbuf, sendcount, sendtype,
  > recvbuf, recvcount, recvtype, comm, ierror)
  do i = 1, sendcount
    read sendbuf(i)
  end do
  do i = 1, recvcount
    write recvbuf(i)
  end do
end subroutine
\end{verbatim}
}
\caption{Memory side effect of $MPI\_Alltoall()$ with simple datatype to override its original definition}
\label{fig:annot:a2a}
\end{figure}
