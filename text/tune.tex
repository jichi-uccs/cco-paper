\section{Program Transformation}
\label{sec-opt}

After being selected by our automatic compiler analysis component, each loop 
to be optimized includes at least one MPI communication $c$ inside the loop body, and two sets of statements, 
Before(c) and After(c), that are safe to be overlapped with $c$ across loop iterations. If the Loop body contains 
multiple MPI communications, say $c1$ followed by $c2$, both of which are selected for optimization, then $c1$ and $c2$
must be independent across different loop iterations, as $c1 \in Before(c2)$ and $c2 \in After(c1)$ must hold, and to be
both selected  for optimization, we must have $c1(I)$ (the MPI communication c1 at loop iteration I) and $After(c1,I-1)$ (the statements after c1 at loop iteration I-1) being independent, and  similarly $c2(I)$ and $Before(c2,I+1)$  being independent. 
Consequently, $c1$ and $c2$ must be independent of each other across loop iterations. 
So they can be optimized one after another without violating any dependence constraints, with 
each optimization considering only a single MPI communication and its surrounding loop, discussed in the following.

For each MPI communication and its surrounding loop to optimize, we currently manually
transform the source code to enable the overlapping of
computation and communication. The main difficulty of fully automating the optimization is to identify the 
actual statements corresponding to $Comm(I)$, $Before(I)$, and $After(I)$ respectively, as shown in Figure~\ref{fig:cco:reorder}(a),
when they are scattered across multiple procedures,  and some of these procedures cannot be inlined in the compiler analysis phase, e.g., due to 
the need for code path specialization as demonstrated in Figure~\ref{fig:annot:ft}. 
Otherwise, our future work will automate the following currently manually applied steps. 

%we first outline the computation and
%communication inside the loop into separate functions, in order to
%make it easier to replicate and reorder them later into different loop
%iterations. 
%In particular, we divide the statements at each iteration
%I of the target loop into the MPI communications at iteration I
%(Comm(I)), the computation (Before(I)) that should run before Comm(I),
%and the computation (After(I)) to evaluate after Comm(I) Each group of
%statements is then outlined into a separate procedure, with the loop
%index variables as its function parameters.\footnote{These components
%  can alternatively be simply tagged as Comm(I), Before(I), and
%  After(I) if the optimization were to be fully automated; outlining
%  makes it easy to modify the code manually.}  Take NAS FT in
%Figure~\ref{fig:ft_loop} as an example.  The loop to optimize is
%divided into $Comm(I)$, the MPI communication at iteration I;
%$Before(I)$, the computation before communication at iteration I; and
%$After(I)$, the computation after communication at iteration I.

\subsection{Converting MPI communications}

Convert each blocking MPI operation, for example, {\em alltoall} collectives
and point-to-point send-receives, in Comm(I) to an equivalent
nonblocking communication combined with a blocking wait, as illustrated in converting
Figure~\ref{fig:cco:reorder:a} to~\ref{fig:cco:reorder:b}.
%For example,
%in Figure~\ref{fig:ft_loop}, the outlined communication function
%$Comm(I)$, which invokes $MPI\_Alltoall$ internally, is replaced by
%$Icomm(I)$ and $Wait(I)$, which are the corresponding nonblocking
%communication ($MPI\_Ialltoall$) and wait operations converted from
%$Comm(I)$, respectively.


\subsection{Reordering computation and communication}

Tag each statement in the loop body as belonging to $Before(I)$,
$After(I)$, $Icomm(I)$, or $Wait(I)$, where $I$ is the index
variable of the surrounding loop, as shown in Figure~\ref{fig:cco:reorder}(b).
Then, \emph{interleave} $Icomm(I)$ with $Before(I+1)$ and $After(I-1)$, as illustrated in
Figure~\ref{fig:cco:reorder:d}, in two steps:

\begin{enumerate}

\item Move $Before(1)$ and $Icomm(1)$ to the outside before the first
  iteration of the loop starts, and move $Wait(N)$ and $After(N)$
  outside after the last loop iteration as shown in
  Figure~\ref{fig:cco:reorder:c}.

\item Move $Before(I)$ and $Icomm(I)$ above $Wait(I-1)$ and
  $After(I-1)$ as shown in Figure~\ref{fig:cco:reorder:d}.

\end{enumerate}
Figure~\ref{fig:cco:shift} shows the new scheduling of the various computation and communication components after the reordering.
 Note that for each iteration $I$, $Before(I)$, $Icomm(I)$, $Wait(I)$ and $After(I)$ are evaluated in the same order as in the original
 computation. The main difference is that $After(I-1)$ and $Before(I+1)$ are now placed in between $Icomm(I)$ and $Wait(I)$, 
 so that the non-blocking communication $Icomm(I)$ can now be processed in parallel with $After(I-1)$ and $Before(I+1)$,
 thereby facilitating overlapping of computation and communication. If $Before(I+1)$ or $After(I-1)$ contains other MPI communications
that are independent of $comm(I)$, the same rescheduling modification can be applied to these communications in the same fashion, where $Icomm(I)$ and $wait(I)$ would belong to the $Before(I)$ or $After(I)$ components of these new communications. 


\begin{figure}
{\scriptsize
  \centering
  \begin{subfigure}[b]{.25\textwidth}
\begin{verbatim}
DO I = 1 .. N
  Before(I)
  Comm(I)
  After(I)
END DO
\end{verbatim}
\vspace{-.1in}
    \caption{Input loop}
    \label{fig:cco:reorder:a}
    \vspace{.1in}
  \end{subfigure}
  \begin{subfigure}[b]{.25\textwidth}
\begin{verbatim}
DO I = 1 .. N
  Before(I)
  Icomm(I)
  Wait(I)
  After(I)
END DO
\end{verbatim}
\vspace{-.1in}
    \caption{Decouple blocking comm}
    \label{fig:cco:reorder:b}
    \vspace{.1in}
  \end{subfigure}
  \begin{subfigure}[b]{.25\textwidth}
\begin{verbatim}
Before(1)
Icomm(1)
DO I = 2 .. N
  Wait(I - 1)
  After(I - 1)
  Before(I)
  Icomm(I)
END DO
Wait(N)
After(N)
\end{verbatim}
\vspace{-.1in}
    \caption{Move first and last iterations}
    \label{fig:cco:reorder:c}
    \vspace{.1in}
  \end{subfigure}
  \begin{subfigure}[b]{.25\textwidth}
\begin{verbatim}
Before(1)
Icomm(1)
DO I = 2 .. N
  Before(I)
  Wait(I - 1)
  Icomm(I)
  After(I - 1)
END DO
Wait(N)
After(N)
\end{verbatim}
\vspace{-.1in}
    \caption{Interleave consecutive iterations}
    \label{fig:cco:reorder:d}
  \end{subfigure}
\caption{Steps to reorder communication and computation}
\label{fig:cco:reorder}
}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.49\textwidth]{fig/ft_shift.png}
\caption{Overlapped computation and communications}
\label{fig:cco:shift}
\end{figure}

\subsection{Replicating the communication buffer}

Each MPI operation needs a dedicated buffer to hold the data being
communicated.  Applications typically first allocate the necessary
communication buffers at the initialization stage and then reuse the
same buffers in the MPI operations across different loop
iterations.  After applying our optimization, as illustrated in
Figure~\ref{fig:cco:shift}, the communication ($Icomm(i)$ and
$Wait(i)$) at each $i$th iteration, where $i \geq 2$, is overlapped
with computation $Before(i+1)$ and $After(i-1)$.  Assuming that two
distinct buffers, {\em InBuf} and {\em OutBuf}, are used for sending
and receiving each message, respectively, each buffer needs to be
replicated into a pair of equal size to ensure that distinct buffers
are used across the overlapping iterations, as illustrated in
Figure~\ref{fig:cco:dup}.  In particular, we replicate each buffer by
allocating additional memory outside the loop and then alternately use
a distinct buffer in every pair of consecutive loop iterations.



\subsection{Inserting MPI\_Tests}

When using nonblocking MPI operations, some CPU time needs to be
allocated, by embedding $MPI\_Test$ calls in the local computation, to
ensure continuous progress of the communications.  If the local
computation is not inside a loop, we insert one or more $MPI\_Test$
calls evenly distributed into the computation.  On the other hand, if
the local computation is inside a loop, we insert $MPI\_Test$ into the
beginning of the loop body and use a conditional variable to adjust
its frequency.  The inserted code is illustrated in
Figure~\ref{fig:cco:test}.  In both cases, the frequency of
$MPI\_Test$ is empirically adjusted as the application is ported to
each architecture.

\begin{figure}
{\scriptsize
  \centering
  \begin{subfigure}[b]{.4\textwidth}
\begin{verbatim}
Before(1, InBuf)
Icomm(1, InBuf)
DO I = 2 .. N
  Before(I, InBuf)
  Wait(I - 1)
  Icomm(I, InBuf, OutBuf)
  After(I - 1, OutBuf)
END DO
Wait(N, OutBuf)
After(N, OutBuf)
\end{verbatim}
    \caption{Original code using one pair of input/output buffers}
    \label{fig:cco:dup:a}
  \end{subfigure}
  \vspace{.01in}
  \begin{subfigure}[b]{.4\textwidth}
\begin{verbatim}
Before(1, InBuf)
Icomm(1, InBuf)
DO I = 2 .. N
  Before(I, I % 2 ==  1 ? InBuf : InBuf2)
  Wait(I - 1)
  Icomm(I, I % 2 ==  0 ? InBuf : InBuf2
         , I % 2 ==  0 ? OutBuf : OutBuf2)
  After(I - 1, , I % 2 ==  1 ? InBuf : InBuf2)
END DO
Wait(N, N % 2 == 1 ? OutBuf : OutBuf2)
After(N, N % 2 == 1 ? OutBuf : OutBuf2)
\end{verbatim}
    \caption{After replicating the input/output buffers}
    \label{fig:cco:dup:b}
  \end{subfigure}
\caption{Replicate buffers for nonblocking communication}
\label{fig:cco:dup}
}
\end{figure}

\begin{figure}[h]
{\scriptsize
\begin{verbatim}
DO I = 1 ... L
  If I % Freq == 0
    MPI_Test
  Original_computation_statements
END DO
\end{verbatim}
}
\caption{Insert MPI\_Test into a loop at specific frequency $Freq$}
\label{fig:cco:test}
\end{figure}

