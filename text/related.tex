\section{Related Work}
\label{sec-related}

Existing work on enhancing the efficiency of MPI applications has 
focused mostly on communication mechanisms underlying the many 
MPI operations, for example, alternative protocols for point-to-point
communications~\cite{brightwell:eurompi03,denis:eurompi11}, collective
operations~\cite{traff:eurompi14:ocd,traff:eurompi14:mcd,graham:eurompi08,mittal:ppopp12},
remote direct memory access
(RDMA)~\cite{liu:ics03,woodall:eurompi06,hatanaka:eurompi13}, load
balancing~\cite{nian:niss09,kale:eurompi14}, and the
elimination of redundant communications through software caching and
the exploitation of data
locality~\cite{buntinas:icpp09,isujita:eurompi14,ozog:ics13}.  In
contrast, our work focuses on application-level performance
enhancement by enabling automated overlapping of MPI communications
with independent local computations.

Iancu et al.~\cite{iancu:ppopp07} tried to automatically determine
message sizes and schedules for MPI communications through an
analytical model of system scale and load to avoid network congestion.
Danalis et al.~\cite{danalis:ics09} investigated compiler
optimizations to potentially automate the overlapping of 
computations and MPI communications, by formulating the side effects of key MPI operations so that
an MPI-aware compiler can automatically assess the safety of several
optimizations, which were then manually applied in their paper.
Various patterns of computation-communication overlapping and
automated optimization schemes have also been
discussed~\cite{danalis:sc05,fishgold:ipdps06}.  Our work also aims to automate the optimization
of MPI applications, by automatically determining the safety of reordering optimizations through compiler 
dependence analysis and the profitability of optimizations from 
analytical performance modeling. Our work particularly focuses on automatically enabling 
a special form of loop-based
communication-computation overlapping in scientific applications. 

%To reason about the profitability of optimizing MPI applications,
Sancho et al.~\cite{sancho:sc06} combined empirical tuning with
networking models to quantify the potential benefit of overlapping
communication and computation in large-scale scientific applications.
Potluri et al.~\cite{potluri:ics10} empirically quantified the
overlapping of MPI-2 operations in a seismic modeling application.  Hu
et al.~\cite{hu:npc08,song:ppopp14} identified the consumer-producer
model from the control flow graph of the application to guide
optimization decisions for overlapping Alltoall communication in a 3-D
FFT.  Didelot et al.~\cite{didelot:imc14,didelot:eurompi12} developed
a message progression model based on collaborative polling that allows
an efficient auto-adaptive overlapping of communication phases with
computation.  In this paper, we predicted the most time-consuming
code paths that contain MPI communications to
optimize, using existing analytical models of
communications~\cite{loggp}.

Preissl et al.~\cite{preissl:tms10} summarized common communication
patterns in MPI applications to enable automated optimization.
Pellegrini et al.~\cite{pellegrini:eurompi12} proposed an exact
dependence analysis approach for increasing the overlapping of
computation and communication.  Subotic et al.~\cite{subotic:hipeac08}
speculatively extracted runtime data-flow to understand the dynamic
dependence of the application.  Aananthakrishnan et
al.~\cite{aananthakrishnan:ics13} used a hybrid static and runtime
data-flow analysis of MPI programs.  We also use dependence analysis
to determine the correctness of optimization, enhanced with additional
knowledge from developers about their applications.

In order to find the optimal placement of nonblocking MPI operations
within the computation control flow, accurate modeling of the
underlying computation and communication is
required~\cite{brightwell:ics04}.  Hoefler et
al.~\cite{hoefler:icppw05} presented an analytical approach to model
MPI barriers.  Ino et al.~\cite{ino:ppopp2001} presented a parallel
computational model for synchronization analysis in MPI.  Martinez et
al.~\cite{martinez:ipdps09} developed an analytical model extending
LogGP~\cite{loggp} for accurate estimation of individual MPI
communication.  Moritz and Frank~\cite{moritz:tpds01} modeled network
contention in MPI applications.  In our optimization, we first
reposition each pair of local computation and nonblocking
communication as far apart as safety allows across different loop
iterations and then insert MPI\_Test with empirically tuned frequencies
into the local computation to ensure proper progress of the
nonblocking communication.
